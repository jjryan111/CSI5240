{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "9b7f82c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "Accuracy is: 0.9963333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2071.1 : 1.0\n",
      "                      :) = True           Positi : Negati =    993.6 : 1.0\n",
      "                     bam = True           Positi : Negati =     21.7 : 1.0\n",
      "                 welcome = True           Positi : Negati =     20.6 : 1.0\n",
      "                     sad = True           Negati : Positi =     20.1 : 1.0\n",
      "                    poor = True           Negati : Positi =     17.0 : 1.0\n",
      "                     ugh = True           Negati : Positi =     15.0 : 1.0\n",
      "               community = True           Positi : Negati =     14.3 : 1.0\n",
      "                followed = True           Negati : Positi =     14.2 : 1.0\n",
      "                  arrive = True           Positi : Negati =     14.2 : 1.0\n",
      "None\n",
      "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis courtesy of https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "# Trendline for dates thanks to https://stackoverflow.com/questions/70852986/add-trend-line-to-datetime-matplotlib-line-graph\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import snscrape\n",
    "import pandas as pd\n",
    "import re, string, random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.dates as dates\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "\n",
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a5055b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cnt = 0\n",
    "tweet_cnt = 0\n",
    "tweet_classed = pd.DataFrame(columns=['file','tweet','sent'])\n",
    "graph_data = []\n",
    "for root, dirs, files in os.walk('/Users/jj/PyProjects/CSI4240/monday.com_hack_data/',topdown=False):\n",
    "    for file in files:\n",
    "        if(file.endswith('.json') and os.stat(os.path.join(root, file)).st_size > 0):\n",
    "            pos_cnt=0\n",
    "            df = pd.read_json(os.path.join(root, file), lines=True)\n",
    "            file.replace('text-query-tweets','')\n",
    "            file.replace('.json','')\n",
    "            for i in range(len(df)):\n",
    "                tweet_classed.at[tweet_cnt, ['file']] = file\n",
    "                tweet_classed.at[tweet_cnt, ['tweet']] = df.at[i,'content']\n",
    "                custom_tokens = remove_noise(word_tokenize(df.at[i,'content']))\n",
    "                tweet_classed.at[tweet_cnt, ['tokenized']] = ' '.join([str(item) for item in remove_noise(word_tokenize(df.at[i,'content']))])\n",
    "                tweet_classed.at[tweet_cnt, ['sent']] = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "                if classifier.classify(dict([token, True] for token in custom_tokens)) == 'Positive':\n",
    "                    pos_cnt = pos_cnt +1\n",
    "                tweet_cnt = tweet_cnt + 1\n",
    "            result = pos_cnt/len(df)\n",
    "            graph_data.append((file,result))\n",
    "tweet_classed.to_csv('/Users/jj/PyProjects/CSI4240/monday.com_hack_data/classed_tweets_data1.csv', index=False)\n",
    "df = pd.DataFrame(graph_data, columns=['d', 'value'])\n",
    "df = df.replace(to_replace='text-query-tweets',value='',regex=True)\n",
    "df = df.replace(to_replace='.json',value='',regex=True)\n",
    "df['d'] = pd.to_datetime(df['d'],format='%Y-%m-%d')\n",
    "df.to_csv('/Users/jj/PyProjects/CSI4240/monday.com_hack_data/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d0d48c0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>d</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-04-05</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-04-13</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-04-29</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-04-09</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-04-25</td>\n",
       "      <td>0.545455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2021-04-24</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2021-04-08</td>\n",
       "      <td>0.652174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>0.607143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2021-04-12</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2021-04-04</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2021-04-19</td>\n",
       "      <td>0.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2021-03-31</td>\n",
       "      <td>0.812500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2021-04-23</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2021-04-03</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2021-04-15</td>\n",
       "      <td>0.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2021-04-14</td>\n",
       "      <td>0.809524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2021-04-02</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2021-04-22</td>\n",
       "      <td>0.594595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2021-04-18</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2021-04-21</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2021-04-17</td>\n",
       "      <td>0.588235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2021-04-01</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2021-04-16</td>\n",
       "      <td>0.692308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2021-04-20</td>\n",
       "      <td>0.433962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2021-04-11</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2021-04-27</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2021-04-30</td>\n",
       "      <td>0.695652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2021-04-26</td>\n",
       "      <td>0.384615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>0.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2021-04-10</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            d     value\n",
       "0  2021-04-05  0.625000\n",
       "1  2021-04-13  0.464286\n",
       "2  2021-04-29  0.441176\n",
       "3  2021-04-09  0.428571\n",
       "4  2021-04-25  0.545455\n",
       "5  2021-04-24  0.368421\n",
       "6  2021-04-08  0.652174\n",
       "7  2021-04-28  0.607143\n",
       "8  2021-04-12  0.823529\n",
       "9  2021-04-04  0.800000\n",
       "10 2021-04-19  0.428571\n",
       "11 2021-03-31  0.812500\n",
       "12 2021-04-23  0.500000\n",
       "13 2021-04-03  0.800000\n",
       "14 2021-04-15  0.766667\n",
       "15 2021-04-14  0.809524\n",
       "16 2021-04-02  0.583333\n",
       "17 2021-04-22  0.594595\n",
       "18 2021-04-18  0.625000\n",
       "19 2021-04-21  0.600000\n",
       "20 2021-04-17  0.588235\n",
       "21 2021-04-01  0.714286\n",
       "22 2021-04-16  0.692308\n",
       "23 2021-04-20  0.433962\n",
       "24 2021-04-11  0.333333\n",
       "25 2021-04-07  0.500000\n",
       "26 2021-04-27  0.833333\n",
       "27 2021-04-30  0.695652\n",
       "28 2021-04-26  0.384615\n",
       "29 2021-04-06  0.450000\n",
       "30 2021-04-10  0.750000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a18f43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
